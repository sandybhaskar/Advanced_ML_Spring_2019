{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (30%) Training a sequence classifier for MNIST data\n",
    "\n",
    "Let’s train an RNN to classify MNIST images. A convolutional neural network would\n",
    "be better suited for image classification (see Chapter 13), but this makes for a simple\n",
    "example that you are already familiar with. We will treat each image as a sequence of\n",
    "28 rows of 28 pixels each (since each MNIST image is 28 × 28 pixels). \n",
    "Your can design your own cell with some numbers of recurrent neurons (a parameter to tune) with a fully connected layer containing 10 neurons (one per class) connected to the output of the last time step. You can begin with a single layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can you try Multi-layer RNN? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (30%) Time series prediction by RNN.\n",
    "\n",
    "Assume time series is $t * sin(t) / 3 + 2 * sin(t*5)$, the problem is to design a RNN model to generate prediction as closer to the original one. You may sample randomly of 20 consecutive values from the time series, and apply your model to generate 20 predicted values to see how closer of your predict values and the original one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can also try your model with any other new initial  sequence of values to generate more creative new time series. (music generations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (10%) Try LSTM cell to classify MNIST data and compare with previous results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (15%) Words embeddings from data set (http://mattmahoney.net/dc/text8.zip) and plot them. For example, if the model is told that “I drink milk” is a valid sentence, and if it knows that “milk” is close to “water” but far from “shoes,”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (15%) Embedded Reber Grammars\n",
    "\n",
    "Embedded Reber grammars were used by Hochreiter and Schmidhuber in their paper about LSTMs. They are artificial grammars that produce strings such as\n",
    "“BPBTSXXVPSEPE.” Check out Jenny Orr’s nice introduction to this topic.\n",
    "Choose a particular embedded Reber grammar (such as the one represented on Jenny Orr’s page), then train an RNN to identify whether a string respects that\n",
    "grammar or not. You will first need to write a function capable of generating a\n",
    "training batch containing about 50% strings that respect the grammar, and 50%\n",
    "that don’t."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
